{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint, global_step_from_engine\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                  GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
    "\n",
    "from utils import get_dataset, make_logdir\n",
    "\n",
    "__file__ = \"./chatbot_exec_log.txt\"\n",
    "\n",
    "logger = logging.getLogger(__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distributed_scalar(scalar, args):\n",
    "    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "    if args.local_rank == -1:\n",
    "        return scalar\n",
    "    scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n",
    "    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "    return scalar_t.item()\n",
    "\n",
    "\n",
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "        \n",
    "def build_input_from_segments(persona, history, reply, tokenizer, labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if labels:\n",
    "        instance[\"labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Function for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(args, tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
    "\n",
    "    logger.info(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if args.num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(args.num_candidates, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(args.personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2*args.max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        labels = bool(j == num_candidates-1)\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    logger.info(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if args.distributed else None\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(not args.distributed))\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\n",
    "\n",
    "    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_loader, valid_loader, train_sampler, valid_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"\", help=\"Path or url of the dataset. If empty download from S3.\")\n",
    "    parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
    "    parser.add_argument(\"--model_checkpoint\", type=str, default=\"openai-gpt\", help=\"Path, url or short name of the model\")\n",
    "    parser.add_argument(\"--num_candidates\", type=int, default=2, help=\"Number of candidates for training\")\n",
    "    parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous exchanges to keep in history\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--valid_batch_size\", type=int, default=4, help=\"Batch size for validation\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8, help=\"Accumulate gradients on several steps\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=6.25e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--lm_coef\", type=float, default=1.0, help=\"LM loss coefficient\")\n",
    "    parser.add_argument(\"--mc_coef\", type=float, default=1.0, help=\"Multiple-choice loss coefficient\")\n",
    "    parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipping gradient norm\")\n",
    "    parser.add_argument(\"--n_epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--personality_permutations\", type=int, default=1, help=\"Number of permutations of personality sentences\")\n",
    "    parser.add_argument(\"--eval_before_start\", action='store_true', help=\"If true start with a first evaluation before training\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n",
    "    parser.add_argument(\"--fp16\", type=str, default=\"\", help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training (-1: not distributed)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # logging is set to INFO (resp. WARN) for main (resp. auxiliary) process. logger.info => log main process only, logger.warning => log all processes\n",
    "    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "    logger.warning(\"Running process %d\", args.local_rank)  # This is a logger.warning: it will be printed by all distributed processes\n",
    "    logger.info(\"Arguments: %s\", pformat(args))\n",
    "\n",
    "    # Initialize distributed training if needed\n",
    "    args.distributed = (args.local_rank != -1)\n",
    "    if args.distributed:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        args.device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
    "    tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer # cant use Autotokenizer because checkpoint could be a Path\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "\n",
    "    model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "    model = model_class.from_pretrained(args.model_checkpoint)\n",
    "    model.to(args.device)\n",
    "    # Add special tokens if they are not already added\n",
    "    add_special_tokens_(model, tokenizer)\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "\n",
    "    # Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n",
    "    if args.fp16:\n",
    "        from apex import amp  # Apex is only required if we use fp16 training\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16)\n",
    "    if args.distributed:\n",
    "        model = DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "    logger.info(\"Prepare datasets\")\n",
    "    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)\n",
    "\n",
    "    # Training function and trainer\n",
    "    def update(engine, batch):\n",
    "        model.train()\n",
    "        batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "        input_ids, mc_token_ids, labels, mc_labels, token_type_ids = batch\n",
    "        (lm_loss), (mc_loss), *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "            mc_labels=mc_labels, labels=labels\n",
    "        )\n",
    "        loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef) / args.gradient_accumulation_steps\n",
    "        if args.fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
    "        if engine.state.iteration % args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    trainer = Engine(update)\n",
    "\n",
    "    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "    def inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "            input_ids, mc_token_ids, labels, mc_labels, token_type_ids = batch\n",
    "            tokenizer.decode(input_ids[0, -1, :].tolist())\n",
    "            # if we dont send labels to model, it doesnt return losses\n",
    "            lm_logits, mc_logits, *_ = model(\n",
    "                input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "            )\n",
    "            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "            labels_flat_shifted = labels[..., 1:].contiguous().view(-1)\n",
    "            return (lm_logits_flat_shifted, mc_logits), (labels_flat_shifted, mc_labels)\n",
    "    evaluator = Engine(inference)\n",
    "\n",
    "    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    if args.n_epochs < 1:\n",
    "        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    if args.eval_before_start:\n",
    "        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "\n",
    "    # Make sure distributed data samplers split the dataset nicely between the distributed processes\n",
    "    if args.distributed:\n",
    "        trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n",
    "        evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n",
    "\n",
    "    # Linearly decrease the learning rate from lr to zero\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])\n",
    "    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "    # Prepare metrics - note how we compute distributed metrics\n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "               \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "    metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args),\n",
    "                    \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(evaluator, name)\n",
    "\n",
    "    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        pbar = ProgressBar(persist=True)\n",
    "        pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "        evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "        log_dir = make_logdir(args.model_checkpoint)\n",
    "        tb_logger = TensorboardLogger(log_dir)\n",
    "\n",
    "        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n",
    "        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), global_step_transform=global_step_from_engine(trainer)), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "        checkpoint_handler = ModelCheckpoint(log_dir, 'checkpoint', n_saved=3)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n",
    "\n",
    "        torch.save(args, log_dir + '/model_training_args.bin')\n",
    "        getattr(model, 'module', model).config.to_json_file(os.path.join(log_dir, CONFIG_NAME))\n",
    "        tokenizer.save_pretrained(log_dir)\n",
    "\n",
    "    # Run the training\n",
    "    trainer.run(train_loader, max_epochs=args.n_epochs)\n",
    "\n",
    "    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "    if args.local_rank in [-1, 0] and args.n_epochs > 0:\n",
    "        os.rename(os.path.join(log_dir, checkpoint_handler._saved[-1][1]), os.path.join(log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "        tb_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset_path DATASET_PATH]\n",
      "                             [--dataset_cache DATASET_CACHE]\n",
      "                             [--model_checkpoint MODEL_CHECKPOINT]\n",
      "                             [--num_candidates NUM_CANDIDATES]\n",
      "                             [--max_history MAX_HISTORY]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--valid_batch_size VALID_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--lr LR] [--lm_coef LM_COEF] [--mc_coef MC_COEF]\n",
      "                             [--max_norm MAX_NORM] [--n_epochs N_EPOCHS]\n",
      "                             [--personality_permutations PERSONALITY_PERMUTATIONS]\n",
      "                             [--eval_before_start] [--device DEVICE]\n",
      "                             [--fp16 FP16] [--local_rank LOCAL_RANK]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Michael\\AppData\\Roaming\\jupyter\\runtime\\kernel-020e0229-a8f4-4e48-b863-2f301e8c1611.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
