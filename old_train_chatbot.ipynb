{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!$sys.executable -m pip install transformers\n",
    "#!$sys.executable -m pip install spacy\n",
    "#!$sys.executable -m pip install pytorch_pretrained_bert\n",
    "#!$sys.executable -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer\n",
    "\n",
    "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import tqdm\n",
    "import logging\n",
    "\n",
    "from pytorch_pretrained_bert import cached_path\n",
    "\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint, global_step_from_engine\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\chatbot\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens for Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - <bos> to indicate the start of the sequence\n",
    "# - <eos> to indicate the end of the sequence\n",
    "# - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "# - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "# - <pad> as a padding token to build batches of sequences\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "# We can add these special tokens to the vocabulary and the embeddings of the model:\n",
    "tokenizer.set_special_tokens(SPECIAL_TOKENS)\n",
    "model.set_num_special_tokens(len(SPECIAL_TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', 'i', 'like', 'the', 'hackathon', '.', 'i', 'am', 'the', '51ndst', 'member', '.'], ['<speaker1>', 'hello', 'how', 'are', 'you', '?'], ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'], ['<speaker1>', 'great', 'to', 'hear', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# Let's define our contexts and special tokens\n",
    "persona = [[\"i\", \"like\", \"the\", \"hackathon\", \".\"],\n",
    "           [\"i\", \"am\", \"the\", \"51ndst\", \"member\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "words, segments, position, sequence = build_inputs(persona, history, reply)\n",
    "\n",
    "print(sequence)  # Our inputs looks like this:\n",
    "\n",
    "\n",
    "# Tokenize words and segments embeddings:\n",
    "words = tokenizer.convert_tokens_to_ids(words)\n",
    "segments = tokenizer.convert_tokens_to_ids(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distractor, Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a distractor to our previously defined persona, history and reply\n",
    "distractor = [\"sorry\", \"to\", \"hear\", \"that\"]\n",
    "\n",
    "# Build & tokenize inputs ending with our distractor like we did with the gold reply\n",
    "words_distractor, segments_distractor, _, _ = build_inputs(persona, history, distractor)\n",
    "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
    "segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
    "\n",
    "# Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
    "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "lm_distractor = [-1] * len(words_distractor)\n",
    "\n",
    "# Store the position of the last tokens for the next-sentence prediction loss\n",
    "last_token = len(words) - 1\n",
    "last_token_distractor = len(words_distractor) - 1\n",
    "\n",
    "# Now we can pad reply and distractor inputs and targets to the same length\n",
    "padding_length = max(len(words), len(words_distractor))\n",
    "def pad(x, padding):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "(words, words_distractor,\n",
    " segments, segments_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                   for x in (words, words_distractor,\n",
    "                                             segments, segments_distractor)]\n",
    "\n",
    "(lm_targets, lm_distractor) = [pad(x, -1) for x in (lm_targets, lm_distractor)]\n",
    " \n",
    "# And gather reply and distractor inputs to build the input tensors:\n",
    "# words tokens\n",
    "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long)\n",
    "# segment tokens\n",
    "token_type_ids = torch.tensor([[segments, segments_distractor]], dtype=torch.long)\n",
    "# Positions tokens can be automatically created by the model as (0, 1, ..., N)\n",
    "# Last tokens location\n",
    "mc_token_ids = torch.tensor([[last_token, last_token_distractor]], dtype=torch.long)\n",
    "# Language modeling labels\n",
    "lm_labels = torch.tensor([[lm_targets, lm_distractor]], dtype=torch.long)\n",
    "# Next-sentence prediction labels\n",
    "mc_labels = torch.tensor([0], dtype=torch.long)  # Gold reply is 1st (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "lm_loss, mc_loss = model(input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids)\n",
    "\n",
    "# Total loss as a weighted sum\n",
    "lm_coef = 2.0\n",
    "mc_coef = 1.0\n",
    "total_loss = lm_loss * lm_coef + mc_loss * mc_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personachat Import (Conversation Dataset) + Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, dataset_path, dataset_cache):    \n",
    "    dataset_path = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
    "    \n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__\n",
    "    \n",
    "    if dataset_cache and os.path.isfile(dataset_cache):\n",
    "        dataset = torch.load(dataset_cache)\n",
    "    else:\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        # Tokenize and encode the dataset using our loaded GPT tokenizer\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "\n",
    "        dataset = tokenize(dataset)\n",
    "        \n",
    "        torch.save(dataset, dataset_cache)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "personachat = get_dataset(tokenizer, \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\", './dataset_cache')\n",
    "for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if 2 > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(2, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(1):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2*2+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "train_sampler = None\n",
    "valid_sampler = None\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=4, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                  GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_candidates = 2\n",
    "personality_permutations = 1\n",
    "max_history = 2 \n",
    "train_batch_size = 4\n",
    "valid_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = get_dataset(tokenizer, \"\",\"./dataset_cache\")\n",
    "\n",
    "    logger.info(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if number_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(number_candidates, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2*max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    logger.info(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    return train_loader, valid_loader, train_sampler, valid_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function and trainer\n",
    "def update(engine, batch):\n",
    "    model.train()\n",
    "    batch = tuple(input_tensor.to(\"cpu\") for input_tensor in batch)\n",
    "    input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "    (lm_loss), (mc_loss), *_ = model(\n",
    "        input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        mc_labels=mc_labels, lm_labels=lm_labels\n",
    "    )\n",
    "    loss = (lm_loss * 1.0 + mc_loss * 1.0) / 8\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    if engine.state.iteration % 8 == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "def inference(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = tuple(input_tensor.to(\"cpu\") for input_tensor in batch)\n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "        logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "        # if we dont send labels to model, it doesnt return losses\n",
    "        lm_logits, mc_logits, *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        )\n",
    "        lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
    "        lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "        return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
    "    tokenizer_class = OpenAIGPTTokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(\"openai-gpt\")\n",
    "    \n",
    "    model_class = OpenAIGPTDoubleHeadsModel\n",
    "    model = model_class.from_pretrained(\"openai-gpt\")\n",
    "    model.to(\"cpu\")\n",
    "    # Add special tokens if they are not already added\n",
    "    add_special_tokens_(model, tokenizer)\n",
    "    optimizer = AdamW(model.parameters(), lr=6.25e-5, correct_bias=True)\n",
    "    \n",
    "    logger.info(\"Prepare datasets\")\n",
    "    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(tokenizer)\n",
    "    \n",
    "    trainer = Engine(update)\n",
    "    \n",
    "    evaluator = Engine(inference)\n",
    "    \n",
    "    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "    trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "    \n",
    "    # Linearly decrease the learning rate from lr to zero\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, 6.25e-5), (3 * len(train_loader), 0.0)])\n",
    "    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "    \n",
    "    # Prepare metrics - note how we compute distributed metrics\n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "               \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "    metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args),\n",
    "                    \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(evaluator, name)\n",
    "        \n",
    "    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "    if -1 in [-1, 0]:\n",
    "        pbar = ProgressBar(persist=True)\n",
    "        pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "        evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "        log_dir = make_logdir(args.model_checkpoint)\n",
    "        tb_logger = TensorboardLogger(log_dir)\n",
    "\n",
    "        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n",
    "        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), global_step_transform=global_step_from_engine(trainer)), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "        checkpoint_handler = ModelCheckpoint(log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n",
    "\n",
    "        torch.save(args, log_dir + '/model_training_args.bin')\n",
    "        getattr(model, 'module', model).config.to_json_file(os.path.join(log_dir, CONFIG_NAME))\n",
    "        tokenizer.save_pretrained(log_dir)\n",
    "        \n",
    "    trainer.run(train_loader, max_epochs=3)\n",
    "        \n",
    "    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "    if -1 in [-1, 0] and 3 > 0:\n",
    "        os.rename(os.path.join(log_dir, checkpoint_handler._saved[-1][1]), os.path.join(log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "        tb_logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Prepare tokenizer, pretrained model and optimizer.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at C:\\Users\\Administrator\\.cache\\torch\\transformers\\4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at C:\\Users\\Administrator\\.cache\\torch\\transformers\\0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "WARNING:transformers.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at C:\\Users\\Administrator\\.cache\\torch\\transformers\\a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.bd0797be126548711309ad2174d2afb16e3c37e891707667603d85e35a4ad001\n",
      "INFO:transformers.configuration_utils:Model config OpenAIGPTConfig {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at C:\\Users\\Administrator\\.cache\\torch\\transformers\\e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
      "INFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <speaker1> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <speaker2> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:root:Prepare datasets\n",
      "INFO:root:Build inputs and labels\n",
      "INFO:root:Pad inputs and convert to Tensor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 599357280 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-435483510cee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-b6843faec917>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prepare datasets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_sampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_loaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-475362cbd60d>\u001b[0m in \u001b[0;36mget_data_loaders\u001b[1;34m(tokenizer)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPECIAL_TOKENS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mMODEL_INPUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"mc_labels\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n_candidates\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 599357280 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
